# -*- coding: utf-8 -*-
"""Group 11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16r_IdlJ_BF_e5ZEW6bgvBKwEFiILvijb

<a href="https://colab.research.google.com/github/vponkia/LoanApprovalPrediction/blob/main/Complete%20ML%20project" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier

df = pd.read_csv('/content/drive/MyDrive/ML/Project/LoanPrediction.csv')
df.head()

"""Getting number of rows and columns."""

print('Number of rows are 614 and columns are 12')
df.shape

df.info()

"""### Treating Null values"""

df.isnull().sum()

"""Most frequently occurring value for Gender, Married, Dependents, Self_Employed, Loan_Amount_Term and Credit_History for categorical feature"""

# Specify the columns for which you want to calculate the mode
columns = ["Gender", "Married", "Dependents", "Self_Employed", "Loan_Amount_Term", "Credit_History"]

modes = {}  # Dictionary to store the mode values

for column in columns:
    mode_value = df[column].mode()[0]
    modes[column] = mode_value
    df[column].replace(np.nan, mode_value, inplace=True) #replace null values with that columns mode_value

# Print the updated Data
print(modes)
print(df.head())

"""Filling in the missing values with mean for numeric value"""

df["LoanAmount"].fillna(df["LoanAmount"].mean(),inplace=True)

print('No more missing values')
df.isnull().sum()

df = df.drop(['Loan_ID'],axis =1)
df.head()

df.info()

"""### Exploratory Data Analysis

Eligibility for loan based on gender that is
Males are taking more loan than Females.
"""

sns.countplot(x ='Gender',hue = 'Loan_Status',data = df)

"""The person who is graduated are more capable to get loan compared to undergraduated."""

sns.countplot(x ='Education',hue = 'Loan_Status',data = df)

"""The Married people are taking loan more than unmarried people."""

sns.countplot(x ='Married',hue = 'Loan_Status',data = df)

"""Independent are more likely to get loan then dependents."""

sns.countplot(x ='Dependents',hue = 'Loan_Status',data = df)

"""The person who have good credit history are capable to get loan"""

sns.countplot(x ='Credit_History',hue = 'Loan_Status',data = df)

sns.countplot(x ='Property_Area',hue = 'Loan_Status',data = df)

df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome']
sns.distplot(df['Total_Income'])

sns.distplot(df['LoanAmount'])

sns.distplot(df['Loan_Amount_Term'])

df = df.drop('Total_Income', axis=1)
df.head()

"""### Creating Dummy variable for all categorical columns"""

categorical_columns = df.select_dtypes(include=['object']).columns
categorical_columns

# Get categorical columns
categorical_columns = df.select_dtypes(include=['object'])

# Create dummy variables for all categorical columns
data_dummies = pd.DataFrame()

for column in categorical_columns:
    # if df[column].nunique() > 2:
        # dummies = pd.get_dummies(df[column], prefix=column)
    #if it has more than 2 values then dropping first generated column because possible values are only 0 and 1
    # else:
    dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)
    data_dummies = pd.concat([data_dummies, dummies], axis=1)

#print generated dummy variables
print(data_dummies.columns)

#concating dummy columns with original dataset
df = pd.concat([df, data_dummies], axis = 1)

#dropping original categorical columns because already generated dummy columns
df.drop(categorical_columns, axis=1, inplace = True)

# print updated dataset
print(df.columns)

df.info()

df.head()

"""### Heatmap

Credit history is more correlated with target column(Loan_status).
"""

plt.figure(figsize=[20,10])
sns.heatmap(df.corr(),annot=True);

"""### Feature and target column"""

X = df.iloc[:,:-1]
y = df.iloc[:,-1]

X

y

"""### Train-test split"""

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.2,random_state=42)
print(xtrain.shape)
print(xtest.shape)
print(ytrain.shape)
print(ytest.shape)

"""### Standardizing the data"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_scaled = sc.fit_transform(xtrain)
X_test_scaled = sc.transform(xtest)

"""### Logistic Regression

Applying Logistic Regression
"""

model = LogisticRegression()
LR = model.fit(X_train_scaled, ytrain)

print("Intercept:", LR.intercept_.round(2))
print("Coefficient:", LR.coef_.round(2))

y_pred_prob = LR.predict_proba(X_test_scaled)
y_pred= LR.predict(X_test_scaled)

df = pd.DataFrame({"Prob_class_0": y_pred_prob[:,0], "Prob_class_1": y_pred_prob[:,1], "Predicted class": y_pred})
df.head()

from sklearn import metrics
print('RMSE:', np.sqrt(metrics.mean_squared_error(ytest, y_pred)))
print('R2:', metrics.r2_score(ytest, y_pred))

Accuracy = np.mean(y_pred == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

font = {
    'weight' : 'bold',
    'size'   : 15}
plt.rc('font', **font)

cm = confusion_matrix(ytest, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [0, 1])
disp.plot(xticks_rotation=45)
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.show()

Metrics = pd.DataFrame({"Accuracy":[accuracy_score(ytest, y_pred, normalize=True)]
,"Precision":[precision_score(ytest, y_pred, average = 'macro')]
,"Recall":[recall_score(ytest, y_pred, average = 'macro')]
,"F1-Score":[f1_score(ytest, y_pred, average = 'macro')]})

Metrics

"""### KNN

Applying KNN Classifier
"""

knn = KNeighborsClassifier()

knn.fit(X_train_scaled, ytrain)
y_pred = knn.predict(X_test_scaled)

Accuracy = np.mean(y_pred == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

Error_Rate = []

for i in range(1,30):

    knn_opt = KNeighborsClassifier(n_neighbors = i)
    knn_opt.fit(X_train_scaled,ytrain)
    y_pred_opt = knn_opt.predict(X_test_scaled)
    Accuracy = np.mean(y_pred_opt == ytest)
    error_rate_value = 1 - Accuracy
    Error_Rate.append(error_rate_value)

plt.figure(figsize=(10,8))
plt.plot(range(1, 30), Error_Rate, 'm', lw = 3,  marker ='^', markerfacecolor ='g', markersize = 10)
plt.title('Finding Optimal Value of K', fontsize = 25)
plt.xlabel('Number of Nearest Neighbours ( k )', fontsize = 15)
plt.ylabel('Error Rate', fontsize = 15)

knn_7 = KNeighborsClassifier(n_neighbors = 7)

knn_7.fit(X_train_scaled, ytrain)
y_pred_7 = knn_7.predict(X_test_scaled)

Accuracy = np.mean(y_pred_7 == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

font = {
    'weight' : 'bold',
    'size'   : 15}
plt.rc('font', **font)

cm = confusion_matrix(ytest, y_pred_7)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [0, 1])
disp.plot(xticks_rotation=45)
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.show()

Metrics = pd.DataFrame({"Accuracy":[accuracy_score(ytest, y_pred_7, normalize=True)]
,"Precision":[precision_score(ytest, y_pred_7, average = 'macro')]
,"Recall":[recall_score(ytest, y_pred_7, average = 'macro')]
,"F1-Score":[f1_score(ytest, y_pred_7, average = 'macro')]})
Metrics

"""### Random forest"""

rf = RandomForestClassifier(n_estimators=10,max_features='sqrt', bootstrap = True, oob_score = True, random_state=101)
rf.fit(xtrain, ytrain)
y_predRF = rf.predict(xtest)

Accuracy = np.mean(y_predRF == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

font = {
    'weight' : 'bold',
    'size'   : 15}
plt.rc('font', **font)

cm = confusion_matrix(ytest, y_predRF)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [0, 1])
disp.plot(xticks_rotation=45)
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.show()

#Estimating Number of Estimator
errors = []

for i in range(1,50):
    rfc = RandomForestClassifier( n_estimators = i, bootstrap=True, max_features = 5)

    rfc.fit(xtrain,ytrain)
    preds = rfc.predict(xtest)
    err = 1 - accuracy_score(preds,ytest)
    n_missed = np.sum(preds != ytest)
    errors.append(err)

plt.figure(figsize = (12,8))
plt.plot(range(1,50),errors, 'g', lw = 3, marker = "^", markersize = 10, markerfacecolor = 'm')
plt.xlabel('Number of Estimaters( Trees )',fontsize = 20)
plt.ylabel('Errors',fontsize = 20)
plt.show()

rf30 = RandomForestClassifier(n_estimators=15,max_features='sqrt', bootstrap = True, oob_score = True, random_state=101)
rf30.fit(xtrain,ytrain)

y_pred_30= rf30.predict(xtest)
Accuracy = np.mean(y_pred_30 == ytest) * 100
print("Avg accuracy : %2.2f" % (Accuracy))

#Confusion Metrix
font = {
    'weight' : 'bold',
    'size'   : 15}
plt.rc('font', **font)

cm = confusion_matrix(ytest, y_pred_30)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [0, 1])
disp.plot(xticks_rotation=45)
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.show()

Metrics = pd.DataFrame({"Accuracy":[accuracy_score(ytest, y_pred_30, normalize=True)]
,"Precision":[precision_score(ytest, y_pred_30, average = 'macro')]
,"Recall":[recall_score(ytest, y_pred_30, average = 'macro')]
,"F1-Score":[f1_score(ytest, y_pred_30, average = 'macro')]})
Metrics

"""### Decesion Tree"""

DT = DecisionTreeClassifier(random_state=42)
DT.fit(xtrain, ytrain)

y_predD = DT.predict(xtest)

Accuracy = np.mean(y_predD == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

font = {
    'weight' : 'bold',
    'size'   : 15}
plt.rc('font', **font)
cm = confusion_matrix(ytest, y_predD)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [0, 1])
disp.plot(xticks_rotation=45)
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.show()

from sklearn.tree import plot_tree
plt.figure(figsize=(24,16),dpi=150)
plot_tree(DT, filled = True, feature_names = X.columns);

"""Max depth = 12"""

DT12 = DecisionTreeClassifier(max_depth=12, random_state=42)
DT12.fit(xtrain, ytrain)
y_predD12 = DT12.predict(xtest)

Accuracy = np.mean(y_predD12 == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

"""Max depth = 8"""

DT11 = DecisionTreeClassifier(max_depth=8, random_state=42)
DT11.fit(xtrain, ytrain)
y_predD11 = DT11.predict(xtest)

Accuracy = np.mean(y_predD11 == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

font = {
    'weight' : 'bold',
    'size'   : 15}
plt.rc('font', **font)
cm = confusion_matrix(ytest, y_predD12)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [0, 1])
disp.plot(xticks_rotation=45)
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.show()

Metrics = pd.DataFrame({"Accuracy":[accuracy_score(ytest, y_predD11, normalize=True)]
,"Precision":[precision_score(ytest, y_predD11, average = 'macro')]
,"Recall":[recall_score(ytest, y_predD11, average = 'macro')]
,"F1-Score":[f1_score(ytest, y_predD11, average = 'macro')]})
Metrics

"""### Neural Network"""

# Create an instance of the neural network model
classifier =  MLPClassifier(max_iter=50, random_state=42)

# Train the MLP classifier using the scaled training data
classifier.fit(X_train_scaled, ytrain)

# Make predictions on the scaled test data
y_pred_MLP = classifier.predict(X_test_scaled)

Accuracy = np.mean(y_pred_MLP == ytest) * 100
print("Accuracy : %2.2f" % (Accuracy))

font = {
    'weight' : 'bold',
    'size'   : 15}
plt.rc('font', **font)

cm = confusion_matrix(ytest, y_pred_MLP)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [0, 1])
disp.plot(xticks_rotation=45)
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.show()

Metrics = pd.DataFrame({"Accuracy":[accuracy_score(ytest, y_pred_MLP, normalize=True)]
,"Precision":[precision_score(ytest, y_pred_MLP, average = 'macro')]
,"Recall":[recall_score(ytest, y_pred_MLP, average = 'macro')]
,"F1-Score":[f1_score(ytest, y_pred_MLP, average = 'macro')]})
Metrics

"""### Feature Importance"""

pd.DataFrame(index = X.columns, data=DT.feature_importances_, columns=['Feature Importance'])